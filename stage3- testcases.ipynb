{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases for functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases for function of mutual_info_score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "def mutual_info(labels_true,labels_pred):\n",
    "    return metrics.mutual_info_score(labels_true, labels_pred)  \n",
    "def test_mutual_info():\n",
    "    labels_true = [0, 0,1,1,1]\n",
    "    labels_pred = ['A','C','A','B','C']\n",
    "    assert mutual_info(labels_true,labels_pred) == 0.11849392256130004\n",
    "\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.parametrize(\"test_input1,test_input2 , expected_output\",[([0, 0,1,1,1],['A','C','A','B','C'],0.11849392256130004),([0, 1,1,1,2],[1,4,3,4,2],0.9502705392332346\n",
    ")])\n",
    "\n",
    "\n",
    "def test_answer(test_input1, test_input2 , expected_output):\n",
    "    result = mutual_info(test_input1,test_input2)\n",
    "    assert  result == expected_output\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases for function of Target variable calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def target(funding_2008,funding_2009):\n",
    "    l = len(funding_2008)\n",
    "    z=[]\n",
    "    for i in range(0,l):\n",
    "        diff=((funding_2009[i]-funding_2008[i])/funding_2009[i])*100\n",
    "      \n",
    "        if(diff>=0):\n",
    "            diff=1\n",
    "        else:\n",
    "            diff=0\n",
    "        z.append(diff)\n",
    "    return z \n",
    "   \n",
    "def test_target():\n",
    "    a = [7.8,1.8,1.1,11.6,1.42,2]\n",
    "    b = [7,1.8,1.1,13,1.347,1.961]\n",
    "    assert target(a,b) == [0, 1, 1, 1, 0, 0]\n",
    "\t\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.parametrize(\"test_input1,test_input2 , expected_output\",[([7.8,1.8,1.1,11.6,1.42,2],[7,1.8,1.1,13,1.347,1.961],[0, 1, 1, 1, 0, 0]),([2.564,3.995,0.3,1.2025,0.237,2.505],[2.525,3.48,0.3,3.113,0.362,1.913],[0, 0, 1, 1, 1, 0]\n",
    ")])\n",
    "\n",
    "\t\n",
    "def test_answer(test_input1, test_input2 , expected_output):\n",
    "    result = target(test_input1,test_input2)\n",
    "    assert  result == expected_output\n",
    "\n",
    "\t\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases for function of label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_encoding(z):\n",
    "    a = {'col1': z}\n",
    "    new_df = pd.DataFrame(data=a)\n",
    "    t_u = new_df['col1'].unique()\n",
    "    l = len(t_u)\n",
    "    new_label = []\n",
    "    for i in range(1,l+1):\n",
    "        new_label.append(i)\n",
    "    for i in range(0,l):\n",
    "        new_df.iloc[list(new_df['col1'] == t_u[i]),0]= new_label[i]\n",
    "    z_new = list(new_df['col1'])   \n",
    "    return z_new\n",
    "\n",
    "\n",
    "def test_label_encoding():\n",
    "    z = [\"c\",\"b\",\"c\",\"a\",\"c\",\"c\",\"b\",\"b\"]\n",
    "    result = label_encoding(z)\n",
    "    assert result == [1, 2, 1, 3, 1, 1, 2, 2]\n",
    "\t\n",
    "\t\n",
    "import pytest\n",
    "@pytest.mark.parametrize(\"test_input , expected_output\",[([\"c\",\"b\",\"c\",\"a\",\"c\",\"c\",\"b\",\"b\"],[1, 2, 1, 3, 1, 1, 2, 2]),( [\"mango\",\"egg\",\"egg\",\"apple\",\"orange\",\"apple\",\"mango\",\"egg\"],[1, 2, 2, 3, 4, 3, 1, 2]\n",
    ")])\n",
    "\n",
    "\t\n",
    "def test_answer(test_input , expected_output):\n",
    "    result = label_encoding(test_input)\n",
    "    assert  result == expected_output\n",
    "\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases for function of calculation frequencies of different categories in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def frequency(d):\n",
    "    d = {'col1': d}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    t_u = df['col1'].unique()\n",
    "    l = len(t_u)\n",
    "    freq =[]\n",
    "    labels = []\n",
    "    for i in range(0,l):\n",
    "        x=df.loc[df['col1'] == t_u[i]]\n",
    "        freq.append(len(x))\n",
    "        labels.append(t_u[i])\n",
    "    return freq\n",
    " \n",
    "\n",
    "def test_frequency():\n",
    "    a = [1,1,1,1,2,2,2,2,3,3,4,5,5] \n",
    "    assert frequency(a) == [4, 4, 2, 1, 2]\n",
    "\n",
    "\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.parametrize(\"test_input , expected_output\",[([1,1,1,1,2,2,2,2,3,3,4,5,5],[4, 4, 2, 1, 2]),(['A','B','C','A','B'],[2,2,1]\n",
    ")])\n",
    "\n",
    "\n",
    "def test_answer(test_input , expected_output):\n",
    "    result = frequency(test_input)\n",
    "    assert  result == expected_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
